{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain, PromptTemplate, HuggingFacePipeline\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.chains import SequentialChain\n",
    "import PyPDF2\n",
    "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import main\n",
    "\n",
    "main.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_id=\"nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\"\n",
    "repo_id=\"google/flan-t5-large\"\n",
    "# repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id=repo_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "Text{text}\n",
    "You are a expert MCQ maker. Given the above test, it is your job to \\\n",
    "create a quiz of {number} multiple choice questions for {subject} students in {tone} tone.\n",
    "Make sure the question are not repeated and checks all the questions to be conforming the text as well.\n",
    "Make sure to format your responsse like RESPONSE_JSON below adn use it as guide. \\\n",
    "Ensure to make {number} MCQS\n",
    "*** RESPONSE_JSON\n",
    "{response_json}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=['text', 'number', 'subject', \"tone\", 'response_json'],\n",
    "    template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain = LLMChain(llm=llm(repo_id=repo_id, model_kwargs={'temperature':0}), prompt=quiz_generation_prompt, output_key='quiz', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english gramarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the questions and give a complete analysis of the quiz. Only use at max 50 words for complexity if the quiz is not at per with the congitive and analytical abilities of the students.\\\n",
    "Update the quiz questions which needs to be changed the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert  english writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt=PromptTemplate(\n",
    "    input_variables=[\"subject\", \"quiz\"],\n",
    "    template=TEMPLATE2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain = LLMChain(llm=llm(repo_id=repo_id, model_kwargs={'temperature':0}), prompt=quiz_evaluation_prompt, output_key='review', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain=SequentialChain(chains=[quiz_chain, review_chain], input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "                                        output_variables=[\"quiz\", \"review\"], verbose=True,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r\"../data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize  the Python dictionary into a JSON-formatted string\n",
    "\n",
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import json\n",
    "\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        self.input_tokens = 0\n",
    "        self.output_tokens = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        print(f\"Total Input Tokens: {self.input_tokens}\")\n",
    "        print(f\"Total Output Tokens: {self.output_tokens}\")\n",
    "\n",
    "    def add_input_tokens(self, count):\n",
    "        self.input_tokens += count\n",
    "\n",
    "    def add_output_tokens(self, count):\n",
    "        self.output_tokens += count\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_id = \"google/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the function\n",
    "# TEXT = \"Translate this text to german.\"\n",
    "NUMBER = 5\n",
    "SUBJECT = \"Machine Learning\"\n",
    "TONE = \"simple\"\n",
    "# RESPONSE_JSON = {\"example_key\": \"example_value\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input data\n",
    "input_data = {\n",
    "    \"text\": TEXT,\n",
    "    \"number\": NUMBER,\n",
    "    \"subject\": SUBJECT,\n",
    "    \"tone\": TONE,\n",
    "    \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "}\n",
    "\n",
    "def process_translation(input_data):\n",
    "    input_text = input_data[\"text\"]\n",
    "\n",
    "    # Define the maximum length for each chunk and output\n",
    "    max_chunk_length = 512\n",
    "    max_output_length = 512\n",
    "\n",
    "    # Split the input text into smaller chunks\n",
    "    input_chunks = [input_text[i:i+max_chunk_length] for i in range(0, len(input_text), max_chunk_length)]\n",
    "\n",
    "    responses = []\n",
    "    with TokenTracker() as tracker:\n",
    "        for chunk in input_chunks:\n",
    "            # Tokenize the chunk\n",
    "            input_tokens = tokenizer.encode(chunk, return_tensors=\"pt\", max_length=max_chunk_length, truncation=True)\n",
    "            input_token_count = len(input_tokens[0])\n",
    "            tracker.add_input_tokens(input_token_count)\n",
    "\n",
    "            # Generate output based on the chunk tokens\n",
    "            output_tokens = model.generate(input_tokens, max_length=max_output_length)\n",
    "            output_token_count = len(output_tokens[0])\n",
    "            tracker.add_output_tokens(output_token_count)\n",
    "\n",
    "            # Decode the output tokens to get the response text for the chunk\n",
    "            response_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "            responses.append(response_text)\n",
    "\n",
    "    # Combine all the responses\n",
    "    final_response = \" \".join(responses)\n",
    "    return final_response\n",
    "\n",
    "# Get the response for the input data\n",
    "response_text = process_translation(input_data)\n",
    "\n",
    "# Display the response\n",
    "print(f\"Response Text: {response_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_response(TEXT, NUMBER, SUBJECT, TONE, RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Input Tokens: 8\n",
      "Total Output Tokens: 12\n",
      "Generated Response:     \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# TokenTracker class to track input and output tokens\n",
    "class TokenTracker:\n",
    "    def __init__(self):\n",
    "        self.input_tokens = 0\n",
    "        self.output_tokens = 0\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        print(f\"Total Input Tokens: {self.input_tokens}\")\n",
    "        print(f\"Total Output Tokens: {self.output_tokens}\")\n",
    "\n",
    "    def add_input_tokens(self, count):\n",
    "        self.input_tokens += count\n",
    "\n",
    "    def add_output_tokens(self, count):\n",
    "        self.output_tokens += count\n",
    "\n",
    "# Load the tokenizer and model\n",
    "model_id = \"google/flan-t5-large\"  # Example model, replace with your own\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "# Example usage of the function\n",
    "def generate_response(text, number, subject, tone, response_json):\n",
    "    # Check if any input data is missing\n",
    "    if not all([text, number, subject, tone, response_json]):\n",
    "        raise ValueError(\"All input parameters must be provided.\")\n",
    "\n",
    "    # Tokenize the input text\n",
    "    input_tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Define a fixed maximum length for the model's input\n",
    "    max_length = 512  # You can adjust this value as needed for your model\n",
    "    if input_tokens.size(1) > max_length:\n",
    "        input_tokens = input_tokens[:, :max_length]\n",
    "        print(f\"Warning: Input truncated to {max_length} tokens.\")\n",
    "\n",
    "    with TokenTracker() as tracker:\n",
    "        # Track the number of input tokens\n",
    "        input_token_count = len(input_tokens[0])\n",
    "        tracker.add_input_tokens(input_token_count)\n",
    "\n",
    "        # Generate the output while handling memory errors\n",
    "        try:\n",
    "            output_tokens = model.generate(\n",
    "                input_tokens, \n",
    "                max_length=min(15000, input_token_count + 100),  # Ensure a reasonable max length\n",
    "                num_beams=5,  # Optional: Set number of beams for beam search\n",
    "                early_stopping=True\n",
    "            )\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            raise RuntimeError(\"Out of memory. Try reducing the input size or use a smaller model.\")\n",
    "\n",
    "        # Track the number of output tokens\n",
    "        output_token_count = len(output_tokens[0])\n",
    "        tracker.add_output_tokens(output_token_count)\n",
    "\n",
    "        # Decode the generated output\n",
    "        response_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "# Example data\n",
    "TEXT = TEXT\n",
    "NUMBER = 5\n",
    "SUBJECT = \"Machine Learning\"\n",
    "TONE = \"formal\"\n",
    "RESPONSE_JSON = RESPONSE_JSON\n",
    "\n",
    "# Call the function\n",
    "try:\n",
    "    result = generate_response(TEXT, NUMBER, SUBJECT, TONE, RESPONSE_JSON)\n",
    "    print(\"Generated Response:\", result)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For OpenAI  \n",
    "# How to setup Token Usage  Tracking  in Langchain\n",
    "\n",
    "# with get_openai_callback() as cb:\n",
    "#     response=generate_evaluate_chain(\n",
    "#         {\n",
    "#             \"text\": TEXT,\n",
    "#             \"number\": NUMBER,\n",
    "#             \"subjecy\": SUBJECT, \n",
    "#             \"tone\": TONE,\n",
    "#             \"respose_json\": json.dumps(RESPONSE_JSON)\n",
    "#         }\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
